{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: green;\">**Vektordatenbanken**</span> using <span style=\"color: red;\">**ChromaDB**</span> with Embeddings for Long Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ChromaDB vs FAISS**\n",
    "\n",
    "| **Feature**                 | **FAISS**                    | **ChromaDB**                  |\n",
    "|------------------------------|------------------------------|-------------------------------|\n",
    "| **Storage**                 | Requires separate metadata   | Integrated metadata           |\n",
    "| **Flexibility**             | Static after creation        | Dynamic, supports updates     |\n",
    "| **Ease of Use**             | More complex setup           | Simpler API                   |\n",
    "| **Performance**             | Highly optimized             | Good for medium datasets      |\n",
    "| **Offline Support**         | Yes                          | Yes                           |\n",
    "\n",
    "**So in the end, we have decided to use ChromaDB**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embeddings models**\n",
    "\n",
    "\n",
    "| **Model**               | **Dimensionality** | **Quality**          | **Offline** | **Use Cases**                                | **Speed**      |\n",
    "|--------------------------|--------------------|----------------------|-------------|---------------------------------------------|----------------|\n",
    "| **text-embedding-ada-002** | 1536               | Best-in-class        | No          | Complex queries, large-scale semantic tasks | Moderate (API) |\n",
    "| **All-MiniLM-L6-v2**     | 384                | Good                 | Yes         | Lightweight tasks, semantic search          | Fast           |\n",
    "| **Instructor-XL**        | 768                | Very Good            | Yes         | Knowledge bases, task-specific embeddings   | Moderate       |\n",
    "| **MPNet**                | 768                | Very Good            | Yes         | Context-aware embeddings, multilingual      | Moderate       |\n",
    "| <span style=\"color: green;\">**GTR-T5 (Large)**</span>       | 1024               | Excellent            | Yes         | Cross-domain, large-scale retrieval         | Slower         |\n",
    "| **Sentence-BERT**        | 768                | Very Good            | Yes         | Sentence similarity, classification         | Moderate       |\n",
    "\n",
    "**we decided to use the free and best option which is GTR-T5 (Large)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation üòé**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the needed Libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (4.46.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/10.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.0/10.1 MB 12.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.8/10.1 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 11.7 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.3\n",
      "    Uninstalling transformers-4.46.3:\n",
      "      Successfully uninstalled transformers-4.46.3\n",
      "Successfully installed tokenizers-0.21.0 transformers-4.47.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (0.5.23)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (2.10.3)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (2.2.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (3.7.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (1.29.0)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n",
      "Requirement already satisfied: anyio in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.7.0)\n",
      "Requirement already satisfied: certifi in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.1)\n",
      "Requirement already satisfied: sympy in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.3)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Using cached tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "Successfully installed tokenizers-0.20.3\n",
      "Requirement already satisfied: sentence-transformers in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "Successfully installed tokenizers-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -U\n",
    "!pip install chromadb\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ChromaDB client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "db_path=\"./vektor_DB\"  # I add this code in order to save the db locally\n",
    "client = chromadb.PersistentClient(path=db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load gtr-t5-large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\UNI\\KI und Big Data\\KI-BigData\\tst\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the SentenceTransformer model for embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/gtr-t5-large\") # after some research that was on of the best free SentenceTransformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\"meinungen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_file = \"Daten.csv\"\n",
    "data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because our text are very long I needed to split it into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a really long texts,and that was our second problem , so I split them into smaller chunks.\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/gtr-t5-large\")\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    while len(words) > 0:\n",
    "        chunk = \" \".join(words[:max_tokens])\n",
    "        chunks.append(chunk)\n",
    "        words = words[max_tokens:]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add records to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: AFD_Volksabstimmungen nach Schweizer Modell_chunk_0\n",
      "Insert of existing embedding ID: AFD_Volksabstimmungen nach Schweizer Modell_chunk_0\n",
      "Add of existing embedding ID: AFD_Volksabstimmungen nach Schweizer Modell_chunk_0\n",
      "Insert of existing embedding ID: AFD_Volksabstimmungen nach Schweizer Modell_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_Au√üen- und Verteidigungspolitik_chunk_0\n",
      "Insert of existing embedding ID: AFD_Au√üen- und Verteidigungspolitik_chunk_0\n",
      "Add of existing embedding ID: AFD_Quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_Quote_chunk_0\n",
      "Add of existing embedding ID: AFD_Quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_Quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: AFD_quote_chunk_0\n",
      "Insert of existing embedding ID: AFD_quote_chunk_0\n",
      "Add of existing embedding ID: Die Linke_Mietendeckel_chunk_0\n",
      "Insert of existing embedding ID: Die Linke_Mietendeckel_chunk_0\n",
      "Add of existing embedding ID: FreieWahler_Land- und forstwirtschaftliche Fl√§chen sch√ºtzen_chunk_0\n",
      "Insert of existing embedding ID: FreieWahler_Land- und forstwirtschaftliche Fl√§chen sch√ºtzen_chunk_0\n",
      "Add of existing embedding ID: FreieWahler_Verpflichtendes kostenfreies Kindergartenjahr_chunk_0\n",
      "Insert of existing embedding ID: FreieWahler_Verpflichtendes kostenfreies Kindergartenjahr_chunk_0\n"
     ]
    }
   ],
   "source": [
    "def add_to_collection(party, theme, long_text):\n",
    "    # Skip if long_text is not a string cause there was a nan value in the end that causes a problem, and I needed to start from the beg.\n",
    "    if not isinstance(long_text, str):\n",
    "        print(f\"Skipping invalid data: party={party}, theme={theme}, long_text={long_text}\")\n",
    "        return\n",
    "    \n",
    "    chunks = chunk_text(long_text)  # call the function to chunk long texts \n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        embedding = embedding_model.encode(chunk)\n",
    "        \n",
    "        collection.add(\n",
    "            documents=[chunk],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[{\"party\": party, \"theme\": theme, \"chunk_index\": chunk_index}],\n",
    "            ids=[f\"{party}_{theme}_chunk_{chunk_index}\"],  # creating Unique ID for each chunk\n",
    "        )\n",
    "\n",
    "# Iterate over all rows and add them to ChromaDB\n",
    "for index, row in data.iterrows():\n",
    "    party = row['Partei']\n",
    "    theme = row['Thema']\n",
    "    long_text = row['Meinung']\n",
    "    add_to_collection(party, theme, long_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
