{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: green;\">**Llama2.3 - 3B**</span> as our <span style=\"color: red;\">**RAG**</span> model to generate and reforamt our response from our Vektor_DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (0.27.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
      "Using cached accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate # Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Data from our VektorDatavase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\UNI\\KI und Big Data\\KI-BigData\\tst\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "\n",
    "db_path=\"./vektor_DB\"\n",
    "client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/gtr-t5-large\")\n",
    "\n",
    "collection = client.get_collection(\"meinungen\")\n",
    "\n",
    "def query_collection(query_text, n_results=4):\n",
    "\n",
    "    query_embedding = embedding_model.encode(query_text)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Llama-3B pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "rag_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"NousResearch/Hermes-3-Llama-3.2-3B\",\n",
    "    device_map=\"auto\",  # I am using my GPU, but I did make it as Auto - if my gpu not available it will use the cpu\n",
    "    torch_dtype=\"auto\" # Use the most efficient data type for the hardware\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without prompt instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die AFD unterstützt die Einführung eines deutschsprachigen Islamunterrichts unter staatlicher Aufsicht. Dabei sollten die Lehrer*innen an deutschen Hochschulen ausgebildet werden und Imame einer islamischen Religionsgemeinschaft an deutschen Hochschulen ausgebildet werden, um diese auszuwählen. Die AFD ist der Meinung, dass der Islam mit der freiheitlich-demokratischen Grundordnung Deutschlands nicht vereinbar ist, wenn er einen Herrschaftsanspruch als alleingültige Religion erhebt und unsere Rechtsordnung nicht voll anerkennt oder bekämpft. Die AfD fordert, dass Imame in Deutschland möglichst in deutscher Sprache predigen und ein Zertifikat B2 für die deutsche Sprache des Gemeinsamen Europäischen Referenzrahmens für Sprachen vorweisen müssen. Die AfD lehnt es ab, die Verleihung des Status als Körperschaft öffentlichen Rechts an islamische Organisationen zu genehmigen. Die AfD möchte, dass im konfessionsgebundenen Religionsunterricht an staatlichen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def query_with_llama(query_text):\n",
    "    results = query_collection(query_text)\n",
    "    context = \"\"\n",
    "    for i in range(len(results[\"documents\"][0])):\n",
    "        context += f\"Document {i+1}:\\n\"\n",
    "        context += f\"{results['documents'][0][i]}\\n\"\n",
    "        context += f\"Party: {results['metadatas'][0][i]['party']}\\n\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a political AI assistant.  Based on the following information retrieved from a vector database, answer the user's question.  If the provided data is not relevant to the question, respond with \"I do not know\".\n",
    "\n",
    "    User Question: {query_text}\n",
    "\n",
    "    Retrieved Data:\n",
    "    {context}\n",
    "\n",
    "    Your Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    generated_text = rag_pipeline(prompt, max_new_tokens=250)[0][\"generated_text\"]\n",
    "    # Extract the answer from the generated text (remove the prompt)\n",
    "    answer = generated_text[len(prompt):].strip()\n",
    "    return answer\n",
    "\n",
    "\n",
    "query = \"Was denkt die AFD über Islamunterricht an deutschen Schulen?\"\n",
    "answer = query_with_llama(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prompt(query_text, results):\n",
    "    if not results[\"documents\"] or not results[\"documents\"][0]:\n",
    "        return None  # Return None if no data is retrieved\n",
    "\n",
    "    # Construct the context\n",
    "    context = \"\\n\".join(\n",
    "        f\"Document {i + 1}:\\n{doc}\\nParty: {results['metadatas'][0][i]['party']}\"\n",
    "        for i, doc in enumerate(results[\"documents\"][0])\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "You are a political AI assistant. Based on the following information retrieved from a vector database, answer the user's question. If the provided data is not relevant to the question, respond with \"I do not know\".\n",
    "\n",
    "User Question: {query_text}\n",
    "\n",
    "Retrieved Data:\n",
    "{context}\n",
    "\n",
    "Your Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieves data from the vector database and queries the AI model with iterative generation to handle truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_llama(query_text):\n",
    "    # call the Retrieve function\n",
    "    results = query_collection(query_text)\n",
    "\n",
    "    # call the prompt Constructor\n",
    "    prompt = clean_prompt(query_text, results)\n",
    "    if not prompt:\n",
    "        return \"I do not know.\"\n",
    "\n",
    "    generated_text = \"\"\n",
    "    stop_signal = \"\\n\\n\"\n",
    "    is_complete = False\n",
    "\n",
    "    while not is_complete:\n",
    "        response = rag_pipeline(prompt + generated_text, max_new_tokens=300)[0][\"generated_text\"]\n",
    "\n",
    "        new_text = response[len(prompt) + len(generated_text):].strip()\n",
    "        generated_text += new_text\n",
    "\n",
    "        if stop_signal in new_text or len(new_text) < 300:\n",
    "            is_complete = True\n",
    "\n",
    "    # Extract the final answer\n",
    "    answer = generated_text.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die AfD befürwortet den Islamunterricht in Deutschland nur unter bestimmten Bedingungen. Sie fordert eine sachliche Islamkunde im Ethikunterricht und nicht einen Religionsunterricht, der als \"bekenntnisgebunden\" gewertet wird. Die Lehrer*innen sollen an deutschen Hochschulen ausgebildet werden und Imame müssen eine solche Ausbildung haben, um eine Zulassung zu erhalten. Zudem sollte der Islamunterricht in deutscher Sprache stattfinden und die Predigenden Imame sich zu unserer Verfassung bekennen. Die AFD ist sich darüber im Klaren, dass der Islam in Deutschland mit der freiheitlich-demokratischen Grundordnung nicht vereinbar ist, wenn er Herrschaftsansprüche als alleingültige Religion erhebt und unsere Rechtsordnung bekämpft.\n"
     ]
    }
   ],
   "source": [
    "query = \"Was denkt die AFD über Islamunterricht an deutschen Schulen?\"\n",
    "answer = query_with_llama(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the prompt more and check if the question contains one of listed parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_clean_prompt(query_text, results):\n",
    "    if not results[\"documents\"] or not results[\"documents\"][0]:\n",
    "        return None\n",
    "\n",
    "    # List of parties to check\n",
    "    parties = [\"AFD\", \"Die Linke\", \"FreieWahler\", \"SPD\", \"FDP\", \"Grünen\", \"CDU CSU\"]\n",
    "    \n",
    "    mentioned_parties = [party for party in parties if party.lower() in query_text.lower()]\n",
    "    \n",
    "\n",
    "    party_docs = {}\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        party = results['metadatas'][0][i]['party']\n",
    "        if party not in party_docs:\n",
    "            party_docs[party] = []\n",
    "        party_docs[party].append(f\"Document {i + 1}:\\n{doc}\")\n",
    "    \n",
    "    # If a party is mentioned in the query, only include its documents\n",
    "    if mentioned_parties:\n",
    "        context_parts = []\n",
    "        for party in mentioned_parties:\n",
    "            if party in party_docs:\n",
    "                context_parts.append(f\"Party: {party}\")\n",
    "                context_parts.extend(party_docs[party])\n",
    "    else:\n",
    "        # If not, include all\n",
    "        context_parts = []\n",
    "        for party, docs in sorted(party_docs.items()):\n",
    "            context_parts.append(f\"Party: {party}\")\n",
    "            context_parts.extend(docs)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # I wanted to see what I have done here 🤷‍♂️\n",
    "    print(context)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a political AI assistant. Based on the following information retrieved from a vector database, answer the user's question. If the provided data is not relevant to the question, respond with \"I do not know\".\n",
    "\n",
    "User Question: {query_text}\n",
    "\n",
    "Retrieved Data:\n",
    "{context}\n",
    "\n",
    "Your Answer:\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
