{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: green;\">**Llama2.3 - 3B**</span> as our <span style=\"color: red;\">**RAG**</span> model to generate and reforamt our response from our Vektor_DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (0.27.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\uni\\ki und big data\\ki-bigdata\\tst\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
      "Using cached accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate # Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Data from our VektorDatavase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\UNI\\KI und Big Data\\KI-BigData\\tst\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "\n",
    "db_path=\"./vektor_DB\"\n",
    "client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/gtr-t5-large\")\n",
    "\n",
    "collection = client.get_collection(\"meinungen\")\n",
    "\n",
    "def query_collection(query_text, n_results=4):\n",
    "\n",
    "    query_embedding = embedding_model.encode(query_text)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Llama-3B pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.02s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "rag_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"NousResearch/Hermes-3-Llama-3.2-3B\",\n",
    "    device_map=\"auto\",  # I am using my GPU, but I did make it as Auto - if my gpu not available it will use the cpu\n",
    "    torch_dtype=\"auto\" # Use the most efficient data type for the hardware\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without prompt instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die AFD unterstÃ¼tzt die EinfÃ¼hrung eines deutschsprachigen Islamunterrichts unter staatlicher Aufsicht. Dabei sollten die Lehrer*innen an deutschen Hochschulen ausgebildet werden und Imame einer islamischen Religionsgemeinschaft an deutschen Hochschulen ausgebildet werden, um diese auszuwÃ¤hlen. Die AFD ist der Meinung, dass der Islam mit der freiheitlich-demokratischen Grundordnung Deutschlands nicht vereinbar ist, wenn er einen Herrschaftsanspruch als alleingÃ¼ltige Religion erhebt und unsere Rechtsordnung nicht voll anerkennt oder bekÃ¤mpft. Die AfD fordert, dass Imame in Deutschland mÃ¶glichst in deutscher Sprache predigen und ein Zertifikat B2 fÃ¼r die deutsche Sprache des Gemeinsamen EuropÃ¤ischen Referenzrahmens fÃ¼r Sprachen vorweisen mÃ¼ssen. Die AfD lehnt es ab, die Verleihung des Status als KÃ¶rperschaft Ã¶ffentlichen Rechts an islamische Organisationen zu genehmigen. Die AfD mÃ¶chte, dass im konfessionsgebundenen Religionsunterricht an staatlichen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def query_with_llama(query_text):\n",
    "    results = query_collection(query_text)\n",
    "    context = \"\"\n",
    "    for i in range(len(results[\"documents\"][0])):\n",
    "        context += f\"Document {i+1}:\\n\"\n",
    "        context += f\"{results['documents'][0][i]}\\n\"\n",
    "        context += f\"Party: {results['metadatas'][0][i]['party']}\\n\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a political AI assistant.  Based on the following information retrieved from a vector database, answer the user's question.  If the provided data is not relevant to the question, respond with \"I do not know\".\n",
    "\n",
    "    User Question: {query_text}\n",
    "\n",
    "    Retrieved Data:\n",
    "    {context}\n",
    "\n",
    "    Your Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    generated_text = rag_pipeline(prompt, max_new_tokens=250)[0][\"generated_text\"]\n",
    "    # Extract the answer from the generated text (remove the prompt)\n",
    "    answer = generated_text[len(prompt):].strip()\n",
    "    return answer\n",
    "\n",
    "\n",
    "query = \"Was denkt die AFD Ã¼ber Islamunterricht an deutschen Schulen?\"\n",
    "answer = query_with_llama(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prompt(query_text, results):\n",
    "    if not results[\"documents\"] or not results[\"documents\"][0]:\n",
    "        return None  # Return None if no data is retrieved\n",
    "\n",
    "    # Construct the context\n",
    "    context = \"\\n\".join(\n",
    "        f\"Document {i + 1}:\\n{doc}\\nParty: {results['metadatas'][0][i]['party']}\"\n",
    "        for i, doc in enumerate(results[\"documents\"][0])\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "You are a political AI assistant. Based on the following information retrieved from a vector database, answer the user's question. If the provided data is not relevant to the question, respond with \"I do not know\".\n",
    "\n",
    "User Question: {query_text}\n",
    "\n",
    "Retrieved Data:\n",
    "{context}\n",
    "\n",
    "Your Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieves data from the vector database and queries the AI model with iterative generation to handle truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_llama(query_text):\n",
    "    # call the Retrieve function\n",
    "    results = query_collection(query_text)\n",
    "\n",
    "    # call the prompt Constructor\n",
    "    prompt = clean_prompt(query_text, results)\n",
    "    if not prompt:\n",
    "        return \"I do not know.\"\n",
    "\n",
    "    generated_text = \"\"\n",
    "    stop_signal = \"\\n\\n\"\n",
    "    is_complete = False\n",
    "\n",
    "    while not is_complete:\n",
    "        response = rag_pipeline(prompt + generated_text, max_new_tokens=300)[0][\"generated_text\"]\n",
    "\n",
    "        new_text = response[len(prompt) + len(generated_text):].strip()\n",
    "        generated_text += new_text\n",
    "\n",
    "        if stop_signal in new_text or len(new_text) < 300:\n",
    "            is_complete = True\n",
    "\n",
    "    # Extract the final answer\n",
    "    answer = generated_text.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die AfD befÃ¼rwortet den Islamunterricht in Deutschland nur unter bestimmten Bedingungen. Sie fordert eine sachliche Islamkunde im Ethikunterricht und nicht einen Religionsunterricht, der als \"bekenntnisgebunden\" gewertet wird. Die Lehrer*innen sollen an deutschen Hochschulen ausgebildet werden und Imame mÃ¼ssen eine solche Ausbildung haben, um eine Zulassung zu erhalten. Zudem sollte der Islamunterricht in deutscher Sprache stattfinden und die Predigenden Imame sich zu unserer Verfassung bekennen. Die AFD ist sich darÃ¼ber im Klaren, dass der Islam in Deutschland mit der freiheitlich-demokratischen Grundordnung nicht vereinbar ist, wenn er HerrschaftsansprÃ¼che als alleingÃ¼ltige Religion erhebt und unsere Rechtsordnung bekÃ¤mpft.\n"
     ]
    }
   ],
   "source": [
    "query = \"Was denkt die AFD Ã¼ber Islamunterricht an deutschen Schulen?\"\n",
    "answer = query_with_llama(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the prompt more and check if the question contains one of listed parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_clean_prompt(query_text, results):\n",
    "    if not results[\"documents\"] or not results[\"documents\"][0]:\n",
    "        return None\n",
    "\n",
    "    # List of parties to check\n",
    "    parties = [\"AFD\", \"Die Linke\", \"FreieWahler\", \"SPD\", \"FDP\", \"GrÃ¼nen\", \"CDU CSU\"]\n",
    "    \n",
    "    mentioned_parties = [party for party in parties if party.lower() in query_text.lower()]\n",
    "    \n",
    "\n",
    "    party_docs = {}\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        party = results['metadatas'][0][i]['party']\n",
    "        if party not in party_docs:\n",
    "            party_docs[party] = []\n",
    "        party_docs[party].append(f\"Document {i + 1}:\\n{doc}\")\n",
    "    \n",
    "    # If a party is mentioned in the query, only include its documents\n",
    "    if mentioned_parties:\n",
    "        context_parts = []\n",
    "        for party in mentioned_parties:\n",
    "            if party in party_docs:\n",
    "                context_parts.append(f\"Party: {party}\")\n",
    "                context_parts.extend(party_docs[party])\n",
    "    else:\n",
    "        # If not, include all\n",
    "        context_parts = []\n",
    "        for party, docs in sorted(party_docs.items()):\n",
    "            context_parts.append(f\"Party: {party}\")\n",
    "            context_parts.extend(docs)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # I wanted to see what I have done here ðŸ¤·â€â™‚ï¸\n",
    "    print(context)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a political AI assistant. Based on the following information retrieved from a vector database, answer the user's question. If the provided data is not relevant to the question, respond with \"I do not know\".\n",
    "\n",
    "User Question: {query_text}\n",
    "\n",
    "Retrieved Data:\n",
    "{context}\n",
    "\n",
    "Your Answer:\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
