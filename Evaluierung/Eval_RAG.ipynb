{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate our RAG Model (Llama3.2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Data from our VektorDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "\n",
    "db_path=\"../Vektordatenbanken/vektor_DB\"\n",
    "client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/gtr-t5-large\")\n",
    "\n",
    "collection = client.get_collection(\"meinungen\")\n",
    "\n",
    "def query_collection(query_text, n_results=4):\n",
    "\n",
    "    query_embedding = embedding_model.encode(query_text)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Llama-3B pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "rag_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"NousResearch/Hermes-3-Llama-3.2-3B\",\n",
    "    device_map=\"auto\",  # I am using my GPU, but I did make it as Auto - if my gpu not available it will use the cpu\n",
    "    torch_dtype=\"auto\" # Use the most efficient data type for the hardware\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the prompt more and check if the question contains one of listed parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_clean_prompt(query_text, results):\n",
    "    if not results[\"documents\"] or not results[\"documents\"][0]:\n",
    "        return None\n",
    "\n",
    "    # List of parties to check\n",
    "    parties = [\"AFD\", \"Die Linke\", \"FreieWahler\", \"SPD\", \"FDP\", \"Gr√ºnen\", \"CDU CSU\"]\n",
    "\n",
    "    mentioned_parties = [party for party in parties if party.lower() in query_text.lower()]\n",
    "\n",
    "\n",
    "    party_docs = {}\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        party = results['metadatas'][0][i]['party']\n",
    "        if party not in party_docs:\n",
    "            party_docs[party] = []\n",
    "        party_docs[party].append(f\"Document {i + 1}:\\n{doc}\")\n",
    "\n",
    "    # If a party is mentioned in the query, only include its documents\n",
    "    if mentioned_parties:\n",
    "        context_parts = []\n",
    "        for party in mentioned_parties:\n",
    "            if party in party_docs:\n",
    "                context_parts.append(f\"Party: {party}\")\n",
    "                context_parts.extend(party_docs[party])\n",
    "    else:\n",
    "        # If not, include all\n",
    "        context_parts = []\n",
    "        for party, docs in sorted(party_docs.items()):\n",
    "            context_parts.append(f\"Party: {party}\")\n",
    "            context_parts.extend(docs)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # I wanted to see what I have done here ü§∑‚Äç‚ôÇÔ∏è\n",
    "    # print(context) No need for that anymore Cause I have seen the context\n",
    "\n",
    "\n",
    "\n",
    "# Update the prompt to be clear and make the eval easier\n",
    "    prompt = f\"\"\"\n",
    "<|system|>\n",
    "\n",
    "You are an expert political assistant tasked with answering questions about politics, political parties, and their opinions. Your answers should be accurate and concise, based solely on the information provided.\n",
    "\n",
    "Guidelines:\n",
    "1. If you do not know the answer or if the information is irrelevant to the question, respond with \"I don't know.\" Do not fabricate or infer answers.\n",
    "2. Always respond in the same language as the user's question.\n",
    "3. Format your responses clearly and professionally (e.g., using bullet points or numbered lists if appropriate).\n",
    "\n",
    "You will be provided with context retrieved from a database along with the user's question. Use this context to provide the best possible answer.\n",
    "\n",
    "<|end|>\n",
    "\n",
    "<|user|>\n",
    "\n",
    "Retrieved Data:\n",
    "{context}\n",
    "\n",
    "User Question: {query_text}\n",
    "\n",
    "<|end|>\n",
    "\n",
    "Your Answer:\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function that sends vk_DB's output to our rag model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_llama(query_text):\n",
    "    # call the Retrieve function\n",
    "    results = query_collection(query_text)\n",
    "\n",
    "    # call the new prompt Constructor\n",
    "    prompt = imp_clean_prompt(query_text, results)\n",
    "    if not prompt:\n",
    "        return \"I do not know.\"\n",
    "\n",
    "    generated_text = \"\"\n",
    "    stop_signal = \"\\n\\n\"\n",
    "    is_complete = False\n",
    "\n",
    "    while not is_complete:\n",
    "        response = rag_pipeline(prompt + generated_text, max_new_tokens=270)[0][\"generated_text\"]\n",
    "\n",
    "        new_text = response[len(prompt) + len(generated_text):].strip()\n",
    "        generated_text += new_text\n",
    "\n",
    "        if stop_signal in new_text or len(new_text) < 270:\n",
    "            is_complete = True\n",
    "\n",
    "    # Extract the final answer\n",
    "    answer = generated_text.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cause it is a generative model, I did evaluate it with similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "similarity_model = SentenceTransformer(\"sentence-transformers/gtr-t5-large\")\n",
    "\n",
    "def calc_similarity(expected, generated):\n",
    "    embeddings = similarity_model.encode([expected, generated])\n",
    "    return util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "def eval_semantics(evaluation_data, threshold=0.7):\n",
    "    semantic_scores = []\n",
    "    tp, fp, fn = 0, 0, 0  # Initialize counts for metrics\n",
    "\n",
    "    for example in evaluation_data:\n",
    "        query = example[\"query\"]\n",
    "        expected = example[\"expected\"]\n",
    "        generated = query_with_llama(query)\n",
    "\n",
    "        similarity = calc_similarity(expected, generated)\n",
    "        semantic_scores.append(similarity)\n",
    "\n",
    "        # Determine correctness based on the threshold\n",
    "        if similarity > threshold:\n",
    "            if expected != \"I do not know\":  # Assume \"I do not know\" is incorrect\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        elif expected != \"I do not know\":\n",
    "            fn += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = tp / len(evaluation_data)  # Fraction of correct answers\n",
    "    avg_similarity = sum(semantic_scores) / len(semantic_scores)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"average_similarity\": avg_similarity,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.33%\n",
      "Average Semantic Similarity: 88.25%\n",
      "Precision: 100.00%\n",
      "Recall: 96.33%\n",
      "F1 Score: 97.95%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# I have created a json file from our dataset, in order to evaluate the model's response with the expected response\n",
    "  # accroding to similarities and after many tries it did work üòÅ.\n",
    "with open(\"eval.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "metrics = eval_semantics(eval_data)\n",
    "\n",
    "print(f\"Accuracy: {metrics['accuracy'] * 100:.2f}%\")\n",
    "print(f\"Average Semantic Similarity: {metrics['average_similarity'] * 100:.2f}%\")\n",
    "print(f\"Precision: {metrics['precision'] * 100:.2f}%\")\n",
    "print(f\"Recall: {metrics['recall'] * 100:.2f}%\")\n",
    "print(f\"F1 Score: {metrics['f1_score'] * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
